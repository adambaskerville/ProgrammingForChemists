{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Chemists: Hands on machine learning for chemists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: It is recommended to use Google Colab for this session due to allowing GPU access**\n",
    "\n",
    "Before you start, run the following code box to install several requirements for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install deepchem\n",
    "!pip install --pre deepchem\n",
    "\n",
    "# install kora\n",
    "!pip install kora\n",
    "\n",
    "# import rdkit\n",
    "import kora.install.rdkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning, the buzzphrase of the last 5 years but what is it? why is it important? why is there a session dedicated to it? Is it just applied statistics? We are going to:\n",
    "\n",
    "* Find out what machine learning is and why it is important for scientists.\n",
    "* Tools needed to do it.\n",
    "* How it is used in science.\n",
    "* Find out many of you have done machine learning without realising it.\n",
    "* Predict the solubility of small drug molecules using machine learning.\n",
    "\n",
    "**Aim of this session:**\n",
    "\n",
    "Machine learning is a **vast** and **rapidly** growing field of research built on complex mathematics and statistics; which we will **not** cover. \n",
    "\n",
    "* Instead we are going to explore how you can **use** it to solve real world problems; application over theory. \n",
    "* There is a high probability you will end up using some form of machine learning in your future career, so it is more important you understand how you can use it as opposed to spending months or years understanding the mathematics behind the algorithms. \n",
    "* [The following image](vas3k.com) gives a bird's-eye view of the machine learning world:\n",
    "\n",
    "<center><img src=\"https://i.pinimg.com/736x/05/38/08/0538088de4041bda05c6f1febc99e1bb.jpg\" width=\"auto\" height=\"auto\" /></center>\n",
    "\n",
    "* We already benefit from machine learning in our day to day lives with but a few examples being:\n",
    "\n",
    "    * **Image recognition**: Software trained using ML to scan through hundreds of MRI and CT medical images per second highlighting abnormalities. Computers have long exceeded the accuracy of a human in this field.\n",
    "    * **Medical diagnosis**: Training programs using clinical symptoms to accurately diagnose patients. \n",
    "    * **Speech recognition**: ML is behind the rise of assistant software like Alexa and Siri. \n",
    "    * **Google**: Google is at the forefront, investing huge amounts of money into all facets of ML. It is used in their translation software, Google photos, Google assistant, Google Maps, self driving cars and many more.\n",
    "    * **Financial services**: Used extensively in the finance sector to predict market shifts, customer spending patterns and it can even predict account closures before they occur!\n",
    "    \n",
    "    * Scientists have even used machine learning to [reconstruct images a person has looked at from monitoring their brain activity!](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence, Machine Learning, and Deep Learning. What's the Difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/AI_ML_DL.jpeg\" width=\"auto\" height=\"auto\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no agreed upon, standard definitions for any of these subject fields, but the [Google Machine Learning glossary](https://developers.google.com/machine-learning/glossary#m) provides sensible definitions:\n",
    "\n",
    "**Artificial Intelligence (AI):** A non-human program or model that can solve sophisticated tasks. For example, a program or model that translates text or a program or model that identifies diseases from radiologic images both exhibit artificial intelligence. Formally, machine learning is a sub-field of artificial intelligence. However, in recent years, some organizations have begun using the terms artificial intelligence and machine learning interchangeably.\n",
    "\n",
    "**Machine Learning (ML):** A program or system that builds (trains) a predictive model from input data. The system uses the learned model to make useful predictions from new (never-before-seen) data drawn from the same distribution as the one used to train the model. Machine learning also refers to the field of study concerned with these programs or systems.\n",
    "\n",
    "**Deep Learning (DL):** Is a machine learning technique that constructs artificial neural networks to mimic the structure and function of the human brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a machine learning problem\n",
    "\n",
    "**Most machine learning problems follow a similar implementation and solution pattern:**\n",
    "\n",
    "1. Read in your data. \n",
    "2. Split the data into training and testing data.\n",
    "3. Format the data. \n",
    "4. Fit a model to the training data.\n",
    "5. Test the model on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in Python: TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python remains one of the most popular languages for machine learning; boasting a variety of libraries aimed at its implementation including [**TensorFlow**](https://www.tensorflow.org/); an end-to-end open source machine learning platform developed by Google, available for many programming languages including Python.\n",
    "\n",
    "* First we import tensorflow into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before we continue, various parts of the code in this worksheet will flag up warnings when they are run. **These are not a concern** but are merely a product of having to get various libraries to communicate with one another. \n",
    "* Certain Python libraries depend on specific versions of other Python libraries, so it is sometimes a balancing act to set up the environment to make them all happy. \n",
    "* The warnings usually refer to `deprecation` statements which simply mean they are suggesting you to use newer syntax from the latest release version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everyone wanting to use machine learning, its implementation has become quite high level requiring small amounts of code.\n",
    "* The biggest hurdle is the cost of the hardware to calculate in sensible time.\n",
    "\n",
    "* Since the start of this session **terabytes of data** has been produced world wide which is the key ingredient for AI, ML and DL.\n",
    "    * Volumes of data are **difficult** for standard computers to handle.\n",
    "    * To achieve results within a sensible time frame you often need server racks full of Graphical Processing Units (GPUs) to run everything using massively parallel programming. \n",
    "    * The popularity of video gaming has been the dominant force in the development of faster and bigger GPU technology which has allowed AI, ML and DL to expand so quickly. \n",
    "    * For companies wanting to analyse large volumes of data the bill can be hundreds of thousands of pounds on hardware/software alone plus the power and expertise to run it.\n",
    "\n",
    "### Difference between a CPU and GPU?\n",
    "\n",
    "Here is a practical difference between a CPU and GPU from the Mythbusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('-P28LKWTzrI',  width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU in Google Colab\n",
    "\n",
    "Google Colab allows us to switch between using a CPU and GPU for code execution! \n",
    "\n",
    "* To do this go to:\n",
    "\n",
    "    `Runtime -> Change runtime type`\n",
    "\n",
    "* and select `GPU`. None corresponds to a CPU. \n",
    "\n",
    "* We will now run our first program on a GPU, **a matrix multiplication.**\n",
    "\n",
    "* <font color='red'>**Exercise:** Write the code for the matrix multiplication after the %time command using the `matmul` command from the numpy library:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# define our matrix size\n",
    "mat_size = 1000\n",
    "\n",
    "# create matrices filled with random numbers using numpy\n",
    "mat1_np = np.random.rand(mat_size, mat_size)\n",
    "mat2_np = np.random.rand(mat_size, mat_size)\n",
    "\n",
    "# convert these numpy arrays to tensors for use in tensorflow\n",
    "mat1_tf = tf.convert_to_tensor(mat1_np)\n",
    "mat2_tf = tf.convert_to_tensor(mat2_np)\n",
    "\n",
    "# multiply the numpy matrices and time\n",
    "%time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <font color='red'>**Exercise:** Now do the same thing but using the `matmul` command from tensorflow on the converted numpy matrices:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see the **real time** is much shorter when using the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main branches of classical machine learning, called **supervised** and **unsupervised** learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Consider a mathematical function of the form\n",
    "\n",
    "$$\n",
    "    y = f(x).\n",
    "$$\n",
    "\n",
    "* Conventionally we plug in the input values $x$, into the known function $f$, to calculate $y$. \n",
    "* **Supervised learning** is where you have input values, $x$, and an output value $y$, and use an algorithm to calculate $f$, the mapping function from the input to the output. \n",
    "    * **The majority of practical machine learning uses supervised learning.** The more input and output data supplied, the more accurately $f$ can be defined **in theory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In contrast to supervised learning, **unsupervised learning** is where you only have input data $x$ and no corresponding output variables. \n",
    "    * The goal for unsupervised learning is to model the underlying structure or distribution in the data to learn more about it. \n",
    "    * It is phrased **unsupervised learning** as unlike supervised learning there is no correct answer and the algorithms are left to their own devises tasked with discovering and presenting interesting structure in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning: Artificial Neural Networks\n",
    "\n",
    "A neural network, more properly referred to as an 'artificial' neural network (ANN) to distinguish it from its biological counterpart are inspired by biological neurons in a brain shown via the following image taken from [Machine learning algorithms in boiler plant root cause analysis ](https://www.ee.co.za/article/application-of-machine-learning-algorithms-in-boiler-plant-root-cause-analysis.html): \n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/neuron_node_comparison.jpeg\" width=\"auto\" height=\"auto\" /></center>\n",
    "\n",
    "* An artificial neuron takes inputs, does some mathematics with them and then produces a single output:\n",
    "\n",
    "1. **Each input is first multiplied by a weight:**\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    x_1 \\rightarrow x_1 * w_1 \\\\\n",
    "    x_2 \\rightarrow x_2 * w_2 \\\\\n",
    "    x_3 \\rightarrow x_3 * w_3 \\\\\n",
    "    x_4 \\rightarrow x_4 * w_4 \\\\\n",
    "    x_5 \\rightarrow x_5 * w_5 \\\\\n",
    "    x_6 \\rightarrow x_6 * w_6 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* The inputs $(x_1, x_2, \\ldots , x_n)$ and weights $(w_1, w_2, \\ldots, w_n)$ are real numbers and can be positive or negative.\n",
    "\n",
    "2. **All the weighted inputs are added together and a bias is added which we will call b:**\n",
    "    * We add a bias as this allows our activation function to shift left and right resulting in a better fit, similar to the 'y-intercept' in $y=mx + c$. \n",
    "\n",
    "$$\n",
    "\\text{Sum} = (x_1 * w_1) + (x_2 * w_2) + (x_3 * w_3) + (x_4 * w_4) + (x_5 * w_5) + (x_6 * w_6) + b\n",
    "$$\n",
    "\n",
    "3. **Pass the sum through an activation function:**\n",
    "\n",
    "$$\n",
    "    y = f(\\text{Sum})\n",
    "$$\n",
    "\n",
    "4. The activation function is used to turn an unbounded input into a predictable one. A commonly used activation function is the **sigmoid function**:\n",
    "\n",
    "$$\n",
    "    S(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "5. The activation function is what decides if the artifical neuron fires. If its calculated value is higher than a threshold value it will fire, if not then it will not.\n",
    "\n",
    "* <font color='red'>**Exercise:** Plot the sigmoid function below. What can you say the function does?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-5, 5, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The sigmoid function only outputs numbers in the range $0 \\rightarrow 1$.\n",
    "\n",
    "* Big negative numbers become ~ 0, and big positive numbers become ~ 1.\n",
    "\n",
    "**Simple example:**\n",
    "\n",
    "Consider a simple 2-neuron network that uses the sigmoid activation function and has the following weights:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    x &= [4,5] \\\\\n",
    "    w &= [0, 1] \\\\\n",
    "    b &= 4\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* Multiply the inputs by their weights and sum them not forgetting the bias:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "     &= (x_1 * w_1) + (x_2 * w_2) + b \\\\\n",
    "     &= (4 * 0) + (5 * 1) + 4 \\\\\n",
    "     &= 9\n",
    "\\end{aligned}\n",
    "$$\n",
    "* Now plug into our activation function:\n",
    "\n",
    "$$\n",
    "y = f(9) = 0.99987\n",
    "$$\n",
    "\n",
    "* The neuron outputs 0.99987 given the input $x=[4,5]$.\n",
    "\n",
    "### Hidden layers\n",
    "\n",
    "* A neural network is a bunch of neurons connected together. \n",
    "    * A hidden layer is any layer(s) between the input (first) layer and output (last) layer\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/neural-network-1.png\" width=\"auto\" height=\"auto\" /></center>\n",
    "\n",
    "### How do neural networks learn?\n",
    "\n",
    "* Think about a person throwing a paper ball into a bin.\n",
    "    * The first throw provides feedback on the mass of the paper ball, air resistance, distance to the bin, force of throw etc... If they missed the bin on their first throw, the brain will change how it conducts the second throw using the information it learned from the first throw. \n",
    "    * Neural networks **learn** in exactly the same way, typically by a feedback process called **backpropagation** (\"backprop\" for short). \n",
    "    * Backpropagation involves **comparing the output a network produces with the output it was meant to produce**, and using the difference between them to modify the weights of the connections between the layers in the network, working from the output layers through the hidden layers to the input layers-going backward. \n",
    "    * Given time backpropagation causes the network to learn, reducing the difference between actual and intended output with the intention to make them coincide.\n",
    "\n",
    "* **Training a neural network = trying to minimize the difference between its current value and the expected value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example: Linear Regression, Beer-Lambert law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Regression](https://mathworld.wolfram.com/LinearRegression.html) is a conventional statistical method borrowed by machine learning as a **supervised** learning algorithm where the predicted output is continuous having a constant slope. It is used to predict values such as sales or price; rather than classifying them into categories such as car or tractor. \n",
    "* Linear regression is the simplest machine learning algorithm which a lot of you have encountered before without realising it; **finding the line of best fit.** \n",
    "\n",
    "* Is this really machine learning? Lets check:\n",
    "\n",
    "    * We have the input data, $x$.\n",
    "    * We have the output data, $y$.\n",
    "    * We calculate the gradient and point of intercept of a line, forming a function $f$, which attempts to map $x$ onto $y$  $\\hspace{0.5cm}\\therefore \\hspace{0.5cm} y = f(x) $.\n",
    "\n",
    "* Linear regression provides us the mapping function $f$, which we can then use to predict output values we have not explicitly calculated or measured. \n",
    "* **Lots of you have been doing machine learning without even realising it!** \n",
    "\n",
    "* We are going to solve a modified version of a problem some of you encountered in Excel workshop 4 of Maths and Data Analysis for Chemists.\n",
    "\n",
    "**Question:**\n",
    "\n",
    "UV/vis spectrophotometry is often used to determine concentrations of metal ions in solution. In order to do this, knowledge of the molar extinction coefficient is needed. This is the absorbance per unit concentration at a given wavelength. A student has made a series of standard samples of KMnO$_4$ of known concentrations, so as to determine the molar extinction coefficient using the Beer-Lambert law\n",
    "\n",
    "$$\n",
    "    A = \\epsilon c l,\n",
    "$$\n",
    "where $A$ is the measured absorbance, $\\epsilon$ is the molar extinction coefficient, $c$ is the concentration and $l$ is the path length of the spectrophotometer cell (1 cm). The spectrometer is known to suffer from a constant offset, hence the following equation should be better model the experimental data:\n",
    "\n",
    "$$\n",
    "    A = \\epsilon c l + A_0,\n",
    "$$\n",
    "\n",
    "Using the data, predict the absorbance when the concentration is 0.7 mM.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "This question is asking for a line of best fit to be fitted to the data set, noting that the equation of a straight line has the same form as the Beer-Lambert law\n",
    "\n",
    "$$\n",
    "    \\begin{array}{cccccc}\n",
    "        & \\underbrace{A} & = & \\underbrace{\\epsilon c l} & + & \\underbrace{A_0} \\\\\n",
    "        & \\downarrow && \\downarrow && \\downarrow\\\\\n",
    "        & y & = & mx & + & c \n",
    "    \\end{array}\n",
    "$$ \n",
    "\n",
    "The original data set consisted of 10 data points but I have generated 200 data points for the purposes of this example, stored in `UV_vis_data.csv`. We will read this file in using pandas and fit a line of best fit to it using two techniques\n",
    "\n",
    "1. **No machine learning:** We use pandas to read in the `.csv` file and Using [`numpy.polyfit`](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) which fits polynomials to data sets, where we specify the order of polynomial as `1`, a straight line:\n",
    "\n",
    "* <font color='red'>**Exercise:** Complete the following code block to find the line of best fit and predict the absorbance:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns # we call this library which is based on matplotlib\n",
    "\n",
    "# set a dark grid style for the plot (looks nice!)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# create the figure (fig) and axes (ax) objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# define the column names in the data file\n",
    "colnames=['concentration', 'absorbance']\n",
    "\n",
    "# specify the location of the dataset\n",
    "data_url = \"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/data/UV_vis_data.csv\"\n",
    "\n",
    "# read the data file into a Pandas DataFrame and assign the column names\n",
    "uv_dat = pd.read_csv(, names=colnames)\n",
    "\n",
    "# use numpy to fit a line of best fit using 'polyfit'\n",
    "m, c = np.polyfit(uv_dat['concentration'], uv_dat[''], 1)\n",
    "\n",
    "# create a numpy array of x-values in order to plot the line of best fit\n",
    "x = np.arange(uv_dat['concentration'].min(),uv_dat['concentration'].max(),  0.001)\n",
    "\n",
    "# define how the y-values are calculated using the equation of a straight line\n",
    "y = m* + c\n",
    "\n",
    "# define the axes labels\n",
    "ax.set_xlabel('Concentration / mM')\n",
    "ax.set_ylabel('Absorbance / a.u.')\n",
    "\n",
    "# plot a scatter plot of the data\n",
    "plt.scatter(uv_dat['concentration'], uv_dat['absorbance'], s=5, color='blue');\n",
    "\n",
    "# plot the line of best fit\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "# declare a value of x, the concentration, for the line of best fit to use and produce a corresponding value of the absorbance\n",
    "conc =\n",
    "\n",
    "# print the predicted value of absorbance given an absorbance of 0.7 a.u. \n",
    "print(\"predicted absorbance = \", m*conc + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Machine learning:** Using a neural network:\n",
    "\n",
    "    * We call the neural network from the **Keras** sub-library using the `sequential` function.\n",
    "        * **Keras** is a wrapper built on top of TensorFlow making it more accessible and easier to work with.\n",
    "    * We want to use 1 hidden layer -> `keras.layers.Dense(units=1, input_shape=[1])`.\n",
    "    * We need to **compile** our model to be run and specify the loss measurement as the mean squared difference algorithm.\n",
    "    * We specify the optimizer which is going to change our weights for us. Here we use the **Adam** optimizer. (Not me, it abbreviates from adaptive moment estimation).\n",
    "    * We finally call the `fit` function on our model.\n",
    "        * An **epoch** is not the same thing as an iteration, an epoch is a **complete pass of the training data**\n",
    "    * We plot the loss against epoch number\n",
    "    * <font color='red'>**Exercise:** We finally ask the model to predict the absorbance:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# call the sequential model (neural netowrk) using keras\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "# compile the model using a mean-squared loss and adam optimizer\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
    "\n",
    "# We now fit the model to our data, converting them into numpy arrays as we go\n",
    "history = model.fit(np.asarray(uv_dat['concentration']), np.asarray(uv_dat['absorbance']), epochs=20)\n",
    "\n",
    "# plot the loss with increasing epoch number\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss Magnitude\")\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()\n",
    "\n",
    "# declare a value of the concentration\n",
    "conc = \n",
    "\n",
    "# use our model to predict the absorbance\n",
    "print(\"predicted absorbance = \", model.predict([conc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Both the standard linear regression method along with the machine learning implementation produce **nearly identical answers.**\n",
    "\n",
    "* Our next example we will venture into the world of **drug discovery** and use machine learning to predict the solubility of drug molecules. First we will cover some prerequisites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepChem: Open source machine learning for Life Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DeepChem](https://deepchem.io/) is a collection of open source tools for drug discovery, materials science, quantum chemistry, and biology; including **machine learning**. \n",
    "\n",
    "* Application of machine learning to drug discovery is **very big business** [estimated to be worth \\\\$591 million in 2018](https://www.fnfresearch.com/ai-for-drug-discovery-market-by-drug-type)  and is expected to reach a value of around \\\\$12 billion by 2027.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* It is estimated that bringing a new drug to market [costs major pharmaceutical companies at least \\\\$4 billion](https://www.forbes.com/sites/matthewherper/2012/02/10/the-truly-staggering-cost-of-inventing-new-drugs/#3d91a3bd4a94), and can take 10-15 years with less than 10% making it to market.\n",
    "\n",
    "* Machine learning offers efficient understanding of vast amounts of chemical data, allowing for selection of the best drug candidates and predicting their possible properties; **all without even setting foot in a lab**. \n",
    "    * Drug companies like machine learning as it **can save them huge amounts of money and time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of DeepChem\n",
    "\n",
    "The first thing we need learn is how we can represent a wide range of complex chemical structures using a unique metric; SMILES: **S**implified **M**olecular-**I**nput **L**ine-**E**ntry **S**ystem. \n",
    "\n",
    "* These take the form of a single line notation for describing the structure of chemical species using short strings. SMILES strings can be imported by most molecule editors for conversion back into two-dimensional drawings or three-dimensional models of the molecules.\n",
    "\n",
    "[SMILES](https://www.rdkit.org/docs/GettingStartedInPython.html) has thefollowing syntax rules:\n",
    "\n",
    "1. **Atoms and Bonds:**\n",
    "\n",
    "* Atoms are represented by the standard abbreviation of the chemical elements, in square brackets. A bond is represented using one of the following symbols: \n",
    "   \n",
    "| Symbol    | Meaning                 | \n",
    "|:---------:|:------------------------|\n",
    "| -         | Single bond             |   \n",
    "| =         | Double bond             |\n",
    "| #         | Triple bond             |\n",
    "| $         | Quadruple bond          |\n",
    "| :         | Aromatic bond           |\n",
    "| .         | Disconnected structures |\n",
    "\n",
    "* Examples:\n",
    "        \n",
    "| SMILES      |  Chemical formula  | Name            | \n",
    "|:------------|:-------------------|:----------------|\n",
    "|  [NH4+]     | NH$_4^+$           | Ammonium        |   \n",
    "|  [OH-]      | OH$^-$             | Hydroxide anion |\n",
    "| [Na+].[Cl-] | NaCl               | Sodium chloride |\n",
    "| [OH3+]      | H$_3$O$^+$         | Hydronium cation|\n",
    "\n",
    "* Combining atomic symbols and bond symbols allows for simple chain structures to be represented. \n",
    "* The structures that are entered using SMILES are represented without hydrogens. \n",
    "* SMILES software understands the number of possible connections that an atom can have. \n",
    "\n",
    "| SMILES  |  Chemical formula  | Name            | \n",
    "|:--------|:-------------------|:----------------|\n",
    "|  CC     | CH$_3$CH$_3$       | Ethane          |   \n",
    "| C=C     | CH$_2$CH$_2$       | Ethene          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Branches:**\n",
    "\n",
    "    * A branch from a chain is specified by placing the SMILES symbol(s) for the branch between parenthesis `()`. \n",
    "    * The string in parentheses is placed directly after the symbol for the atom to which it is connected. \n",
    "    * If it is connected by a double or triple bond, the bond symbol immediately follows the left parenthesis.\n",
    "\n",
    "| SMILES            |  Name            | \n",
    "|:------------------|:-----------------|\n",
    "|  CC(O)C           | 2-Propanol       |   \n",
    "| CC(C)CC(=O)       | 2-Methylbutanal  |\n",
    "| c1c(N(=O)=O)cccc1 | Nitrobenzene     |\n",
    "\n",
    "\n",
    "3. **Rings:**\n",
    "\n",
    "    * SMILES allows a user to identify ring structures using numbers to identify the opening and closing ring atom. For example, in `C1CCCCC1`, the first carbon has a number '1' which connects by a single bond with the last carbon which also has a number '1'. \n",
    "    * Chemicals that have multiple rings may be identified by using different numbers for each ring. \n",
    "    * If a double, single, or aromatic bond is used for the ring closure, the bond symbol is placed before the ring closure number. \n",
    "\n",
    "| SMILES         |  Name            | \n",
    "|:---------------|:-----------------|\n",
    "|  C=1CCCCC1     | Cyclohexene      |   \n",
    "| c1ccccc1       | Benzene          |\n",
    "| C1OC1CC        | Ethyloxirane     |\n",
    "| c1cc2ccccc2cc1 | Naphthalene      | \n",
    "\n",
    "4. **Charged atoms:**\n",
    "\n",
    "    * Charges on an atom can be used to override the knowledge regarding valence that is built into SMILES software. \n",
    "    * The format for identifying a charged atom consists of the atom followed by brackets which enclose the charge on the atom. \n",
    "\n",
    "| SMILES              |  Name                               | \n",
    "|:--------------------|:------------------------------------|\n",
    "|  CCC(=O)[O-1]       | Ionized form of propanoic acid      |   \n",
    "| c1cccc[n+1]1CC(=O)O | 1-Carboxylmethyl pyridinium         |\n",
    "\n",
    "* <font color='red'>**Exercise:** Pick several structures and draw them using the code box below</font>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kora.install.rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# put some molecular structures in a list\n",
    "smiles_list = ['OCCc1ccn2cnccc12','C1CC1Oc1cc2ccncn2c1','CNC(=O)c1nccc2cccn12']\n",
    "\n",
    "# draw the chemical structures using MolFromSmiles\n",
    "mol_list = [Chem.MolFromSmiles(x) for x in smiles_list]\n",
    "\n",
    "# use MolsToGridImage to put the images in a grid\n",
    "img = Draw.MolsToGridImage(mol_list, molsPerRow=5, subImgSize=(250, 250))\n",
    "\n",
    "# print the structures to screen\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example: Predicting the solubility of small molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An important property of a new candidate drug is its **solubility**; \n",
    "    * If it isn't soluble enough then it will be unlikely to enter a patient's bloodstream to have a therapeutic effect. \n",
    "* We will now use machine learning to build a model that predicts solubility of small molecules based on nothing but their chemical structure. \n",
    "    * We will be using the delaney dataset from [MoleculeNet](http://moleculenet.ai/datasets-1). \n",
    "        * This dataset contains structures and **log-scale water solubility data for 1128 compounds:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset using deepchem\n",
    "import deepchem as dc\n",
    "\n",
    "# load the Delaney dataset\n",
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='GraphConv')\n",
    "\n",
    "# split the data into training and testing data sets\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use a particular kind of neural network called a **graph convolutional network**, or \"graphconv\" for short. \n",
    "\n",
    "* A **graph** in machine learning is a data structure comprising of nodes (vertices) and edges connected together to represent information with no definite beginning or end. Nodes can be thought of as the atoms while the edges represent connections, the bonds.\n",
    "\n",
    "* We specify `n_tasks=1` i.e. there is only one task, one output value (the solubility) for each sample. \n",
    "* We also specify that this is a regression model, meaning that the labels are continuous numbers and the model should try to reproduce them as accurately as possible. \n",
    "    * This is in contrast to a classification model, which tries to predict which of a fixed set of classes each sample belongs to. \n",
    "* To reduce overfitting, we specify `dropout=0.2`, meaning that 20% of the outputs from the hidden layer will randomly be set to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now need to train the model on the data set. We simply give it the data set and tell it how many epochs of training to perform (that is, how many complete passes through the data to make)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We should now have a fully trained model, but first we must evaluate the model on the **test set**. \n",
    "* We do that by calling `evaluate()` on the model. \n",
    "    * For this example, we will use the **pearson correlation**, which is a number between -1 and +1 that indicates to what extent 2 variables are linearly related. \n",
    "    * We can evaluate it on both the training set and test set in order to test for overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "\n",
    "print(\"Training set score:\", model.evaluate(train_dataset, [metric], transformers))\n",
    "print(\"Test set score:\", model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model is overfitting a little, but a correlation coefficient of 0.83 is respectable; so our very quick model can be said to predict the solubilities of molecules based on their molecular structures. **Not too bad for just a few lines of Python code!**\n",
    "\n",
    "* I don't have any new molecules to hand to test our model on, so let's just use the first ten molecules from the test set. \n",
    "* For each one we print out the chemical structure (represented as a SMILES string) and the predicted solubility. \n",
    "* <font color='red'>If you had a molecule you could convert it into a SMILES format and test our model on it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the solubility of 10 molecules from the test data\n",
    "solubilities = model.predict_on_batch(test_dataset.X[:10])\n",
    "\n",
    "# print the solubilities and the corresponding structure\n",
    "for molecule, solubility in zip(test_dataset.ids, solubilities):\n",
    "    print(solubility, molecule)\n",
    "\n",
    "# draw the stuctures\n",
    "mol_list = [Chem.MolFromSmiles(smiles) for smiles in test_dataset.ids[:10]]\n",
    "img = Draw.MolsToGridImage(mol_list, molsPerRow=5)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete solubility program:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset using deepchem\n",
    "import deepchem as dc\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2)\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=100)\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "\n",
    "print(\"Training set score:\", model.evaluate(train_dataset, [metric], transformers))\n",
    "print(\"Test set score:\", model.evaluate(test_dataset, [metric], transformers))\n",
    "\n",
    "# predict the solubility of 10 molecules from the test data\n",
    "solubilities = model.predict_on_batch(test_dataset.X[:10])\n",
    "\n",
    "# print the solubilities and the corresponding structure\n",
    "for molecule, solubility in zip(test_dataset.ids, solubilities):\n",
    "    print(solubility, molecule)\n",
    "\n",
    "# draw the stuctures\n",
    "mol_list = [Chem.MolFromSmiles(smiles) for smiles in test_dataset.ids[:10]]\n",
    "img = Draw.MolsToGridImage(mol_list, molsPerRow=5)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we covered:\n",
    "\n",
    "* The difference between Artificial Intelligence (AI), Machine Learning (ML) and Deep learning (DL).\n",
    "* Hardware required for machine learning.\n",
    "* Difference between a Central Processing Unit (CPU) ans Graphics Processing Unit (GPU)\n",
    "* Doing machine learning using tensorflow in python.\n",
    "* Showed that linear regression is a machine learning algorithm.\n",
    "* Used linear regression to model the Beer-Lambert law.\n",
    "* Learned the basics of the DeepChem library and SMILES notation.\n",
    "* Wrote a program to estimate the solubility of small drug molecules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading:\n",
    "\n",
    "The DeepChem community has put together a series of tutorials on how to use DeepChem including a large variety of worked examples:\n",
    "    \n",
    "[DeepChem Tutorials](https://github.com/deepchem/deepchem/tree/master/examples/tutorials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
