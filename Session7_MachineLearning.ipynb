{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Chemists: How Machine Learning is Changing Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning, the buzzphrase of the last 5 years but what is it? why is it important? why is there a session dedicated to it? Is it just applied statistics? We are going to:\n",
    "\n",
    "* Find out what machine learning is and why it is important.\n",
    "* Tools needed to do it.\n",
    "* How it is used in science.\n",
    "* Find out many of you have done machine learning without realising it.\n",
    "* Predict the solubility of small drug molecules using machine learning.\n",
    "\n",
    "**Aim of this session:**\n",
    "\n",
    "Machine learning is a **vast** and **rapidly** growing field of research built onstatistics complex mathematics and statistics; which we will **not** cover. Instead we are going to explore how you can **use** it to solve real world problems; application over theory. There is a high probability you will end up using some form of machine learning in your future career, so it is more important you understand how you can use it as opposed to spending months or years understanding the mathematics behind the algorithms. [The following image](vas3k.com) gives a bird's-eye view of the machine learning world:\n",
    "\n",
    "<center><img src=\"https://i.pinimg.com/736x/05/38/08/0538088de4041bda05c6f1febc99e1bb.jpg\" width=\"600\" height=\"600\" /></center>\n",
    "\n",
    "We already benefit from machine learning in our day to day lives:\n",
    "\n",
    "* **Image recognition**: Software trained using ML to scan through hundreds of MRI and CT medical images per second highlighting abnormalities. Computers have long exceeded the accuracy of a human in this field.\n",
    "* **Medical diagnosis**: Training programs using clinical symptoms to accurately diagnose patients. \n",
    "* **Speech recognition**: ML is behind the rise of assistant software like Alexa and Siri. \n",
    "* **Google**: Google is at the forefront, investing huge amounts of money into all facets of ML. It is used in their translation software, Google photos, Google assistant, Google Maps and many more.\n",
    "* **Financial services**: Used extensively in the finance sector to predict market shifts, customer spending patterns and it can even predict account closures before they occur!\n",
    "\n",
    "**Philosophy of AI:**\n",
    "\n",
    "Artificial Intelligence is a hotly debated issue in computer science and wider society for good reason; it could be humanities greatest invention but could it be our last? My personal take is more optimistic believing that AI systems capable of learning will only enhance and improve our own capability of learning. If I am wrong, at least this session will give an insight into how the machine overlords will *think*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence, Machine Learning, and Deep Learning. What's the Difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/AI_ML_DL.jpeg\" width=\"400\" height=\"400\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no agreed upon, standard definitions for any of these subject fields, but the [Google Machine Learning glossary](https://developers.google.com/machine-learning/glossary#m) provides sensible definitions:\n",
    "\n",
    "**Artificial Intelligence (AI):** A non-human program or model that can solve sophisticated tasks. For example, a program or model that translates text or a program or model that identifies diseases from radiologic images both exhibit artificial intelligence. Formally, machine learning is a sub-field of artificial intelligence. However, in recent years, some organizations have begun using the terms artificial intelligence and machine learning interchangeably.\n",
    "\n",
    "**Machine Learning (ML):** A program or system that builds (trains) a predictive model from input data. The system uses the learned model to make useful predictions from new (never-before-seen) data drawn from the same distribution as the one used to train the model. Machine learning also refers to the field of study concerned with these programs or systems.\n",
    "\n",
    "**Deep Learning (DL):** Is a machine learning technique that constructs artificial neural networks to mimic the structure and function of the human brain.\n",
    "\n",
    "Machine learning exists at the intersection of computer science, statistics, and data science. It uses elements of each of these fields, processing data in a way that can detect and learn from patterns, predict future activity, or make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Barrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so many companies wanting to use machine learning, its implementation has become quite high level requiring small amounts of code; the main hurdle being the cost of the hardware to calculate in sensible time.\n",
    "\n",
    "AI, ML and DL are mainly based on learning from **very large quantities of data** which standard computational hardware struggles with. To achieve results within a sensible time frame you often need server racks full of Graphical Processing Units (GPUs) to run everything using massively parallel programming. The popularity of video gaming has been the dominant force in the development of faster and bigger GPU technology which has allowed AI, ML and DL to expand so quickly. For companies wanting to analyse large volumes of data the bill can be hundreds of thousands of pounds on hardware/software alone plus the power and expertise to run it.\n",
    "\n",
    "**Difference between a CPU and GPU?**\n",
    "\n",
    "This is a technical topic but NVIDIA hired the Mythbusters to explain the difference more practically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('-P28LKWTzrI',  width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Multiplication, CPU vs. GPU:**\n",
    "\n",
    "I wrote two programs which multiply two randomly generated matrices of specified size, \\\\(\\mathbf{A}\\\\) and \\\\(\\mathbf{B}\\\\)\n",
    "\n",
    "\\\\[\n",
    "    \\mathbf{M} = \\mathbf{A} \\times \\mathbf{B},\n",
    "\\\\]\n",
    "and timed how long it took for the programs to finish. One of these programs was written for the CPU whilst the other was written for the GPU.\n",
    "\n",
    "**CPU:** Intel Xeon W-2145, 3.70GHz, 16 logical cores\n",
    "\n",
    "**GPU:** Titan V, 1.2 MHz, 5120 CUDA cores\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/CPU_vs_GPU_MatrixMult-1.png\" width=\"600\" height=\"600\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turing test: How we test Artificial Intelligence\n",
    "\n",
    "Machine learning is a realisation of the pioneering work of Alan Turing in the 1950s;  where he published multiple papers, one titled \"[Computing Machinery and Intelligence](https://academic.oup.com/mind/article/LIX/236/433/986238)\". In this paper he described \"*The Imitation Game*\" as a means of answering a big question, \"**Can machines think?**\". Being practically minded Turing noted that trying to define **thinking** is a philosophical hurdle so opted for rephrasing the question as \"*Can machines do what we (as thinking entities) can do?*\" drawing a sharp distinction between the physical and intellectual capacities of a human. \n",
    "\n",
    "Turing described his version of the \"*imitation game*\", in which an interrogator attempts to distinguish which of two players is female, and where each player, hidden from sight, attempts to fool the interrogator into thinking they are the female through written answers to the interrogator’s questions. In Turing’s version, the male is replaced by a machine that attempts to respond to questions in such a way as to fool the interrogator into thinking it, not the female player, is female. \n",
    "\n",
    "The imitation game is now more commonly referred to as the **Turing test** and a subtle but vital quality of the test is that *written human language represents human-level thinking*. If the human Turing test judge is competent, then an entity requires *human-level intelligence* in order to pass the test. Modern AI systems must pass this test if they are to be deemed to have human level intelligence and as of November 2020, **nothing has passed the Turing test**, even if some have claimed to. It is predicted that something will pass the Turing test between the years 2029 and 2040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main types of machine learning, called **supervised** and **unsupervised** learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Consider a mathematical function of the form\n",
    "\n",
    "\\\\[\n",
    "    y = f(x).\n",
    "\\\\]\n",
    "\n",
    "Conventionally we plug in the input values \\\\(x\\\\), into the known function \\\\(f\\\\), to calculate \\\\(y\\\\). **Supervised learning** is where you have input values, \\\\(x\\\\), and an output value \\\\(y\\\\), and use an algorithm to calculate \\\\(f\\\\), the mapping function from the input to the output. **The majority of practical machine learning uses supervised learning.** The more input and output data supplied, the more accurately \\\\(f\\\\) can be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to supervised learning, **unsupervised learning** is where you only have input data \\\\(x\\\\) and no corresponding output variables. The goal for unsupervised learning is to model the underlying structure or distribution in the data to learn more about it. It is phrased **unsupervised learning** as unlike supervised learning there is no correct answer and the algorithms are left to their own devises tasked with discovering and presenting interesting structure in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between conventional programming and machine learning\n",
    "These two types of machine learning signify the difference between traditional programming and machine learning. Conventionally we provide input data and the rules needed to transform this input data into a certain output. Supervised machine learning is supplied input data together with the related output data and attempts to figure out the rules that transform the input into the output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in Python: TensorFlow with a worked example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python remains one of the most popular languages for machine learning; boasting a variety of libraries aimed at its implementation including [**TensorFlow**](https://www.tensorflow.org/); an end-to-end open source machine learning platform developed by Google, available for many programming languages including Python.\n",
    "\n",
    "Most machine learning problems follow a similar implementation and solution pattern:\n",
    "\n",
    "1. Read in your data. \n",
    "2. Split the data into training and testing data.\n",
    "3. Format the data. \n",
    "4. Fit a model to the training data.\n",
    "5. Test the model on the testing data.\n",
    "\n",
    "We will learn to use TensorFlow using a simple example, **how to train a computer to recognise handwritten digits**; the \"Hello world\" of the machine learning world. First we import tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a wrapper built on top of TensorFlow making it more accessible and easier to work with. We use it to access the [MNIST data set](http://yann.lecun.com/exdb/mnist/), composed of handwritten digits; 60,000 **training** images and 10,000 **testing** images:\n",
    "\n",
    "**Training set:** A set where we know the outcome of the feature we are modelling.\n",
    "\n",
    "**Testing set:** A set to test the model created from the training set.\n",
    "\n",
    "We load the data set using `mnist = tf.keras.datasets.mnist`. We will also print some of the images to visualise what we are going to train and test on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the mnist data set using keras\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# next we split the data into training and testing data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# For any pixel of the image, the value assigned to it can fall between 0 and 255. We divide by 255 to normalize the range from 0 -> 1 \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# plot the first 5 images using a loop\n",
    "for i in range(5):  \n",
    "    plt.imshow(x_train[i]) # imshow displays an image\n",
    "    plt.colorbar()         # the colorbar shows the intensity of each pixel\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will use features of images in the training set to learn and then make predictions on similar images that it has not seen before. We first assign a model to fit to the data, in this instance using a **sequential model**. A sequential model allows a computer program to model, interpret, make predictions about or generate any type of sequential data, such as audio, text etc... A real world example being language translation software as the type of data being dealt with is text, which is sequential in nature. \n",
    "\n",
    "Sequence models use **neural networks** which take input, processes it in hidden layers using weights that are adjusted during training, and the model then outputs a prediction. The weights are adjusted to find patterns in order to make better predictions. The user does not need to specify what patterns to look for - **the neural network learns on its own**. \n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/neural-network-1.png\" width=\"600\" height=\"600\" /></center>\n",
    "\n",
    "1. Specify sequential model using keras: \n",
    "\n",
    "    `tf.keras.models.Sequential`\n",
    "\n",
    "2. Flatten the images. Remember Tensorflow uses **tensors**, \\\\(n\\\\)-dimensional arrays, and we want to ensure the images are specified as 2D objects. The input layer has 784 neurons, one for each pixel in the photo \\\\((28, 28) \\rightarrow 28 \\times 28 \\rightarrow 784\\\\):\n",
    "\n",
    "    `tf.keras.layers.Flatten(input_shape=(28, 28))`\n",
    "\n",
    "3. Create hidden layer with 512 nodes.\n",
    "\n",
    "    `tf.keras.layers.Dense(512, activation=tf.nn.relu)`\n",
    "\n",
    "4. Regularization with neural networks involves randomly selected neurons being ignored in training. In this example we are dropping one of every five inputs for the next training cycle (0.2 = 20%). This helps to prevent the neural network from overfitting to the training data. If certain neurons are turned off then other neurons need to step in to handle the portrayal of the data in each cycle, resulting in a better model.\n",
    "\n",
    "    `tf.keras.layers.Dropout(0.2)`\n",
    "\n",
    "5. The output layer has 10 neurons, one for each digit image, \\\\(0 \\rightarrow 9\\\\). An activation function is  **softmax** activation function \n",
    "\n",
    "    `tf.keras.layers.Dense(10, activation=tf.nn.softmax)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([                              # 1. specify sequential model\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),            # 2. format the images into a 2D array\n",
    "        tf.keras.layers.Dense(512, activation=tf.nn.relu),        # 3. create a densely connected neural layer with 512 nodes\n",
    "        tf.keras.layers.Dropout(0.2),                             # 4. drop 20% of the neurons each training cycle\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)       # 5. create output layer with10 nodes and use Softmax activation function\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not possible to visualise all the nodes in our model, but the following picture shows the output nodes with a greater number of input and hidden nodes, giving an insight into how neural networks scale:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/neural_network-2.png\" width=\"600\" height=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is Adam me? **No!**. It is not even an acronym but is derived from \"adaptive moment estimation\". Adam is an optimization algorithm which updates network weights based on the training data. The loss function is used to measure the variation between the value being predicted and what it actually is. **The goal is to minimize the loss (error) function.**\n",
    "\n",
    "We can now fit and evaluate our model. An **epoch** is not the same thing as an iteration, an epoch is a **complete pass of the training data**. Having a model run for too few epochs would weaken the models' performance. Having a model run for too many epochs would risk it starting to \"remember\" the outputs it may want compromising its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved an accuracy of ~ 98%  with just a few lines of Python code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model, **let's test on a number which we draw!** I went to paint and drew a number 4 using the paintbrush tool and converted it into the same image format that `mnist` uses. This is important as the model learned on images which had the same normalization and size so it would not describe an image well with a completely different format! My 4 looks like this:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/four_original.png\" width=\"200\" height=\"200\" /></center>\n",
    "\n",
    "after converting it into a \\\\(28 \\times 28 \\\\) pixel image:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/four_converted.png\" width=\"200\" height=\"200\" /></center>\n",
    "\n",
    "We can now test our model on it and see if it can tell us that the number I drew is a 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# this part of the code is only needed in Jupyter. If running locally, cv can read images from your documents without needing requests etc...\n",
    "url = r'https://raw.githubusercontent.com/adambaskerville/ProgrammingForChemists/master/images/four_converted.png'\n",
    "resp = requests.get(url, stream=True).raw\n",
    "image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "image = cv.imdecode(image, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "image = image.astype('float32')\n",
    "image = image.reshape(1, 28, 28, 1)\n",
    "image = 255-image\n",
    "image /= 255\n",
    "\n",
    "# call the predict function on our image\n",
    "pred = model.predict(image.reshape(1, 28, 28, 1), batch_size=1)\n",
    "\n",
    "# print the probability that our image is 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "print(pred)\n",
    "\n",
    "# ask it to print its prediction\n",
    "print(\"\\nPredicted digit drawn is {} with {:.2f}% predicted accuracy\".format(pred.argmax(), pred[0, pred.argmax()]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computer has learned from training data what digits looks like and we supplied it a brand new image of the number 4, and it was able to correctly identify the number with ~98% accuracy. This reveals the predictive power of DL and ML for image recognition which has so many applications to name but a few:\n",
    "\n",
    "1. Cell counting when  culturing cells.\n",
    "2. Looking for abnormalities in medical images.\n",
    "3. Augmented reality.\n",
    "4. Driverless cars.\n",
    "5. Tracking animal numbers and their migration patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example: Linear Regression, Beer-Lambert law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Regression](https://mathworld.wolfram.com/LinearRegression.html) is a conventional statistical method borrowed by machine learning as a **supervised** learning algorithm where the predicted output is continuous having a constant slope. It is used to predict values such as sales or price; rather than classifying them into categories such as car or tractor. Linear regression is the simplest machine learning algorithm which a lot of you have encountered before without realising it; **finding the line of best fit.** \n",
    "\n",
    "Is this really machine learning? Lets check:\n",
    "\n",
    "* We have the input data, \\\\(x\\\\).\n",
    "* We have the output data, \\\\(y\\\\).\n",
    "* We calculate the gradient and point of intercept of a line, forming a function \\\\(f\\\\), which attempts to map \\\\(x\\\\) onto \\\\(y\\\\)  \\\\(\\hspace{0.5cm}\\therefore \\hspace{0.5cm} y = f(x) \\\\).\n",
    "\n",
    "Linear regression provides us the mapping function \\\\(f\\\\), which we can then use to predict output values we have not explicitly calculated or measured. **Machine learning is often confused with applied statistics** but linear regression can be classed as both a statistical method and a machine learning algorithm. Lots of you have been doing machine learning without even realising it! \n",
    "\n",
    "We are going to solve a modified version of a problem some of you encountered in Excel workshop 4 of Maths and Data Analysis for Chemists.\n",
    "\n",
    "**Question:**\n",
    "\n",
    "UV/vis spectrophotometry is often used to determine concentrations of metal ions in solution. In order to do this, knowledge of the molar extinction coefficient is needed. This is the absorbance per unit concentration at a given wavelength. A student has made a series of standard samples of KMnO\\\\(_4\\\\) of known concentrations, so as to determine the molar extinction coefficient using the Beer-Lambert law\n",
    "\n",
    "\\\\[\n",
    "    A = \\epsilon c l,\n",
    "\\\\]\n",
    "where \\\\(A\\\\) is the measured absorbance, \\\\(\\epsilon\\\\) is the molar extinction coefficient, \\\\(c\\\\) is the concentration and \\\\(l\\\\) is the path length of the spectrophotometer cell (1 cm). The spectrometer is known to suffer from a constant offset, hence the following equation should be better model the experimental data:\n",
    "\n",
    "\\\\[\n",
    "    A = \\epsilon c l + A_0,\n",
    "\\\\]\n",
    "\n",
    "Using the data, predict the absorbance when the concentration is 0.7 mM.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "This question is asking for a line of best fit to be fitted to the data set, noting that the equation of a straight line has the same form as the Beer-Lambert law\n",
    "\n",
    "\\\\[\n",
    "    \\begin{array}{cccccc}\n",
    "        & \\underbrace{A} & = & \\underbrace{\\epsilon c l} & + & \\underbrace{A_0} \\\\\n",
    "        & \\downarrow && \\downarrow && \\downarrow\\\\\n",
    "        & y & = & mx & + & c \n",
    "    \\end{array}\n",
    "\\\\] \n",
    "\n",
    "The original data set consisted of 10 data points but I have generated 200 data points for the purposes of this example, stored in `UV_vis_data.csv`. We will read this file in using pandas and fit a line of best fit to it using two techniques\n",
    "\n",
    "1. **No machine learning:** Using [`numpy.polyfit`](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) which fits polynomials to data sets, where we specify the order of polynomial as `1`, a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns # we call this library which is based on matplotlib\n",
    "\n",
    "# set a dark grid style for the plot (looks nice!)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# create the figure (fig) and axes (ax) objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# define the column names in the data file\n",
    "colnames=['concentration', 'absorbance']\n",
    "\n",
    "# read the data file into a Pandas DataFrame and assign the column names\n",
    "uv_dat = pd.read_csv(\"data/UV_vis_data.csv\", names=colnames)\n",
    "\n",
    "# use numpy to fit a line of best fit using 'polyfit'\n",
    "m, c = np.polyfit(uv_dat['concentration'], uv_dat['absorbance'], 1)\n",
    "\n",
    "# create a numpy array of x-values in order to plot the line of best fit\n",
    "x = np.arange(uv_dat['concentration'].min(),uv_dat['concentration'].max(),  0.001)\n",
    "\n",
    "# define how the y-values are calculated using the equation of a straight line\n",
    "y = m*x + c\n",
    "\n",
    "# define the axes labels\n",
    "ax.set_xlabel('Concentration / mM')\n",
    "ax.set_ylabel('Absorbance / a.u.')\n",
    "\n",
    "# plot a scatter plot\n",
    "plt.scatter(uv_dat['concentration'], uv_dat['absorbance'], s=5, color='blue');\n",
    "\n",
    "# plot the x and y-values of the line of best fit\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# declare a value of x, the absorbance, for the line of best fit to use and produce a corresponding value of the concentration\n",
    "conc = 0.7\n",
    "\n",
    "# print the predicted value of concentration given an absorbance of 0.7 a.u. \n",
    "print(m*conc + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Machine learning:** Using a linear regression machine learning algorithm from tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# define the column names in the data file\n",
    "colnames=['concentration', 'absorbance']\n",
    "\n",
    "# read the data file into a Pandas DataFrame and assign the column names\n",
    "uv_dat = pd.read_csv(\"data/UV_vis_data.csv\", names=colnames)\n",
    "\n",
    "# call the sequential model using keras\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "# compile the model using a mean-squared loss error and the Adam optimizer\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
    "\n",
    "history = model.fit(np.asarray(uv_dat['concentration']), np.asarray(uv_dat['absorbance']), epochs=20, verbose=False)\n",
    "\n",
    "# plot the loss function against epoch number\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss Magnitude\")\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()\n",
    "\n",
    "# declare a value of x, the absorbance, for the line of best fit to use and produce a corresponding value of the concentration\n",
    "conc = 0.7\n",
    "\n",
    "print(model.predict([conc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now covered two machine learning examples and for our final example we will venture into the world of **drug discovery** and use machine learning to predict the solubility of drug molecules. First we will cover some prerequisites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepChem: Open source machine learning for Life Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DeepChem](https://deepchem.io/) is a collection of open source tools for drug discovery, materials science, quantum chemistry, and biology; including **machine learning**. Application of machine learning to drug discovery is **very big business** [estimated to be worth \\\\$591 million in 2018](https://www.fnfresearch.com/ai-for-drug-discovery-market-by-drug-type)  and is expected to reach a value of around \\\\$12 billion by 2027.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "It is estimated that bringing a new drug to market [costs major pharmaceutical companies at least \\\\$4 billion](https://www.forbes.com/sites/matthewherper/2012/02/10/the-truly-staggering-cost-of-inventing-new-drugs/#3d91a3bd4a94), and can take 10-15 years with less than 10% making it to market.\n",
    "\n",
    "Machine learning offers efficient understanding of vast amounts of chemical data, allowing for selection of the best drug candidates and predicting their possible properties; **all without even setting foot in a lab**. Drug companies like machine learning as it **can save them huge amounts of money and time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of DeepChem\n",
    "\n",
    "The first thing we need understand is how we can represent a wide range of complex chemical structures using a unique metric; SMILES: **S**implified **M**olecular-**I**nput **L**ine-**E**ntry **S**ystem. These take the form of a single line notation for describing the structure of chemical species using short strings. SMILES strings can be imported by most molecule editors for conversion back into two-dimensional drawings or three-dimensional models of the molecules.\n",
    "\n",
    "[SMILES](https://archive.epa.gov/med/med_archive_03/web/html/smiles.html) has five basic syntax rules:\n",
    "\n",
    "1. **Atoms and Bonds:**\n",
    "| Symbol    | Meaning                 | \n",
    "|:---------:|:------------------------|\n",
    "| -         | Single bond             |   \n",
    "| =         | Double bond             |\n",
    "| #         | Triple bond             |\n",
    "| *         | Aromatic bond           |\n",
    "| .         | Disconnected structures |\n",
    "\n",
    "2. **Chains:**\n",
    "\n",
    "    Combining atomic symbols and bond symbols allows for simple chain structures to be represented. The structures that are entered using SMILES are hydrogen-suppressed, represented without hydrogens. SMILES software understands the number of possible connections that an atom can have. \n",
    "\n",
    "| SMILES  |  Chemical formula  | Name            | \n",
    "|:--------|:-------------------|:----------------|\n",
    "|  CC     | CH$_3$CH$_3$       | Ethane          |   \n",
    "| C=C     | CH$_2$CH$_2$       | Ethene          |\n",
    "| Na.Cl   | NaCl               | Sodium chloride |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# put some molecular structures in a list\n",
    "smiles_list = [\"CC\",\"C=C\",\"[Na+].[Cl-]\"]\n",
    "\n",
    "# draw the chemical structures using MolFromSmiles\n",
    "mol_list = [Chem.MolFromSmiles(x) for x in smiles_list]\n",
    "\n",
    "# use MolsToGridImage to put the images in a grid\n",
    "img = Draw.MolsToGridImage(mol_list, molsPerRow=5, subImgSize=(250, 250), legends=None, useSVG=True)\n",
    "\n",
    "# print the structures to screen\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". 3. **Branches:**\n",
    "\n",
    "    A branch from a chain is specified by placing the SMILES symbol(s) for the branch between parenthesis `()`. The string in parentheses is placed directly after the symbol for the atom to which it is connected. If it is connected by a double or triple bond, the bond symbol immediately follows the left parenthesis.\n",
    "\n",
    "| SMILES            |  Name            | \n",
    "|:------------------|:-----------------|\n",
    "|  CC(O)C           | 2-Propanol       |   \n",
    "| CC(C)CC(=O)       | 2-Methylbutanal  |\n",
    "| c1c(N(=O)=O)cccc1 | Nitrobenzene     |\n",
    "\n",
    "\n",
    "4. **Rings:**\n",
    "\n",
    "    SMILES allows a user to identify ring structures using numbers to identify the opening and closing ring atom. For example, in `C1CCCCC1`, the first carbon has a number '1' which connects by a single bond with the last carbon which also has a number '1'. Chemicals that have multiple rings may be identified by using different numbers for each ring. If a double, single, or aromatic bond is used for the ring closure, the bond symbol is placed before the ring closure number. Some examples:\n",
    "\n",
    "\n",
    "| SMILES         |  Name            | \n",
    "|:---------------|:-----------------|\n",
    "|  C=1CCCCC1     | Cyclohexene      |   \n",
    "| c1ccccc1       | Benzene          |\n",
    "| C1OC1CC        | Ethyloxirane     |\n",
    "| c1cc2ccccc2cc1 | Naphthalene      | \n",
    "\n",
    "5. **Charged atoms:**\n",
    "\n",
    "    Charges on an atom can be used to override the knowledge regarding valence that is built into SMILES software. The format for identifying a charged atom consists of the atom followed by brackets which enclose the charge on the atom. The number of charges may be explicitly stated ({-1}) or not ({-}):\n",
    "\n",
    "| SMILES              |  Name                               | \n",
    "|:--------------------|:------------------------------------|\n",
    "|  CCC(=O)O{-1}       | Ionized form of propanoic acid      |   \n",
    "| c1ccccn{+1}1CC(=O)O | 1-Carboxylmethyl pyridinium         |\n",
    "\n",
    "<font color='red'>Pick several structures and draw them using the code box below</font>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rdkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a4d3dad38e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPythonConsole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msmiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C1=CC(=C(C=C1C2=C(C(=O)C3=C(C=C(C=C3O2)O)O)O)O)O'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rdkit'"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# put some molecular structures in a list\n",
    "smiles_list = []\n",
    "\n",
    "# draw the chemical structures using MolFromSmiles\n",
    "mol_list = [Chem.MolFromSmiles(x) for x in smiles_list]\n",
    "\n",
    "# use MolsToGridImage to put the images in a grid\n",
    "img = Draw.MolsToGridImage(mol_list, molsPerRow=5, subImgSize=(250, 250), legends=None, useSVG=True)\n",
    "\n",
    "# print the structures to screen\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example: Predicting the solubility of small molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important property of a new candidate drug is its solubility; if it isn't soluble enough then it will be unlikely to enter a patient's bloodstream to have a therapeutic effect. We will now use machine learning to build a model that predicts solubility of small molecules based on nothing but their chemical structure. We will be using the delaney dataset from [MoleculeNet](http://moleculenet.ai/datasets-1). This dataset contains structures and log-scale water solubility data for 1128 compounds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset using deepchem\n",
    "import deepchem as dc\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a particular kind of neural network called a \"graph convolutional network\", or \"graphconv\" for short. A **graph** in machine learning is a series of nodes that are connected using a line if they share a relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to train the model on the data set. We simply give it the data set and tell it how many epochs of training to perform (that is, how many complete passes through the data to make)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have a fully trained model, but first we must evaluate the model on the **test set**. We do that by calling `evaluate()` on the model. For this example, we will use the **Pearson correlation**, also known as r2, as our metric. We can evaluate it on both the training set and test set to test for overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "\n",
    "print(\"Training set score:\", model.evaluate(train_dataset, [metric], transformers))\n",
    "print(\"Test set score:\", model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't have any other molecules to hand to test our model on, so let's just use the first ten molecules from the test set. For each one we print out the chemical structure (represented as a SMILES string) and the predicted solubility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solubilities = model.predict_on_batch(test_dataset.X[:10])\n",
    "for molecule, solubility in zip(test_dataset.ids, solubilities):\n",
    "    print(solubility, molecule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete solubility program:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset using deepchem\n",
    "import deepchem as dc\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2)\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=100)\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "\n",
    "print(\"Training set score:\", model.evaluate(train_dataset, [metric], transformers))\n",
    "print(\"Test set score:\", model.evaluate(test_dataset, [metric], transformers))\n",
    "solubilities = model.predict_on_batch(test_dataset.X[:10])\n",
    "for molecule, solubility in zip(test_dataset.ids, solubilities):\n",
    "    print(solubility, molecule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
